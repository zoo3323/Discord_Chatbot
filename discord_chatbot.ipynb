{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 14 15:48:50 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 Ti   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   40C    P8              18W / 290W |    759MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3828    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      6372    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     11080    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11680    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12492    C+G   ... Synapse 3 Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     14712    C+G   ...B\\system_tray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     17708    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     19772    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\Administrator\\miniconda3\\envs\\GPU\\bin\\cudart64_12.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll...\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166705fcf7e24c16b8a9adcb8581558c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(30080, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (query_key_value): Linear4bit(\n",
       "                in_features=4096, out_features=12288, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear4bit(in_features=4096, out_features=16384, bias=True)\n",
       "              (dense_4h_to_h): Linear4bit(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=4096, out_features=30080, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = \"Joo99/discord_chatbot\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    q = f\"### 질문: {x}\\n\\n### 답변:\"\n",
    "    # print(q)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            q, \n",
    "            return_tensors='pt', \n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'), \n",
    "        max_new_tokens=126,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    decoded_answer = tokenizer.decode(gened[0])\n",
    "    answer = decoded_answer.split(\"### 답변:\")[1].split(\"<|끝|>\")[0].split(\"|\")[0].split(\"/\")[0].split(\"\\\\\")[0].strip()\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'너는 오늘 안 움직이니'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"안녕? 오늘 뭐해?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-14 15:53:49] [INFO    ] discord.client: logging in using static token\n",
      "[2023-10-14 15:53:50] [INFO    ] discord.gateway: Shard ID None has connected to Gateway (Session ID: 3ea89e689147d0d3b0904c4bd6e3cdb5).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 폰은정#3114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "[2023-10-14 15:54:41] [WARNING ] discord.gateway: Shard ID None heartbeat blocked for more than 10 seconds.\n",
      "Loop thread traceback (most recent call last):\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\discord\\client.py\", line 441, in _run_event\n",
      "    await coro(*args, **kwargs)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17804\\1858288561.py\", line 19, in on_message\n",
      "    res = gen(req)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17804\\4162475151.py\", line 4, in gen\n",
      "    gened = model.generate(\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\peft\\peft_model.py\", line 764, in generate\n",
      "    outputs = self.base_model.generate(**kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1572, in generate\n",
      "    return self.sample(\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2619, in sample\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py\", line 673, in forward\n",
      "    outputs = self.gpt_neox(\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py\", line 564, in forward\n",
      "    outputs = layer(\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py\", line 331, in forward\n",
      "    attention_layer_outputs = self.attention(\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py\", line 149, in forward\n",
      "    cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1502, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\Administrator\\miniconda3\\envs\\GPU\\lib\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py\", line 278, in forward\n",
      "    return self.cos_cached[:seq_len, ...].to(x.device), self.sin_cached[:seq_len, ...].to(x.device)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# __filename__ == 'test_discordbot.py'\n",
    "import discord\n",
    "from discord.ext import commands\n",
    "\n",
    "discord_bot_token = ''\n",
    "intents = discord.Intents.default()\n",
    "intents.message_content = True\n",
    "bot = commands.Bot(command_prefix=\">\", intents=intents)\n",
    "\n",
    "@bot.event\n",
    "async def on_ready():\n",
    "    print('Bot: {}'.format(bot.user))\n",
    "\n",
    "@bot.event\n",
    "async def on_message(message):\n",
    "\n",
    "    if message.content.startswith('!'):\n",
    "        req = message.content\n",
    "        res = gen(req)\n",
    "        await message.channel.send(res, tts=True)\n",
    "        return\n",
    "\n",
    "    await bot.process_commands(message)\n",
    "bot.run(discord_bot_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
